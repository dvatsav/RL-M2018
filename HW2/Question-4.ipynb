{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "A = (0, 1)\n",
    "B = (0, 3)\n",
    "A1 = (4, 1)\n",
    "B1 = (2, 3)\n",
    "n = 5\n",
    "actions = [(0, 1), (0, -1),(1, 0), (-1,0)]\n",
    "action_names = {'0' : 'r', '1' : 'l', '2' : 'd', '3' : 'u'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* System of Non - Linear Equations. We can modify this to a system of linear inequalities.\n",
    "* We can model Bellman's Equation as a system of linear inequalities with 4 equations for each state, one equation for each \n",
    "* action. We can model this as V*_pi >= R_pi + gamma* P_pi * V*_pi, where P_pi is the state transition matrix. \n",
    "* We can then solve for v_pi* as a linear programming problem\n",
    "* First we will initialize values that make up the MDP\n",
    "\"\"\"\n",
    "\n",
    "R_pi = np.zeros(4*n*n)\n",
    "G = np.zeros((4*n*n, n*n)) # Coefficient matrix of V*_pi\n",
    "\n",
    "# Model as a cost function instead of a reward function. We do this since we are minimizing while solving the LPP problem\n",
    "def step_and_reward(i, j, action):\n",
    "    if (i,j) == A:\n",
    "        return (A1[0]*n+A1[1], -10)\n",
    "    elif (i,j) == B:\n",
    "        return (B1[0]*n+B1[1], -5)\n",
    "    elif 0 <= i + action[0] < n and 0 <= j+action[1] < n:\n",
    "        return ((i + action[0])*n + j+action[1], 0)\n",
    "    else:\n",
    "        return (i*n+j, 1)\n",
    "\n",
    "# Set up rewards and transition matrix\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        for k in range(len(actions)):\n",
    "            next_pos, reward = step_and_reward(i, j, actions[k])\n",
    "            R_pi[4*(i*n+j) + k] += reward\n",
    "            G[4*(i*n+j) + k, i*n+j] -= 1 # mimic an identiy matrix\n",
    "            G[4*(i*n+j) + k, next_pos] += gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0.9,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       [-0.1,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       [-1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       ...,\n",
       "       [ 0. ,  0. ,  0. , ...,  0. ,  0.9, -1. ],\n",
       "       [ 0. ,  0. ,  0. , ...,  0. ,  0. , -0.1],\n",
       "       [ 0. ,  0. ,  0. , ...,  0. ,  0. , -1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = opt.linprog(np.ones(n*n), G, R_pi)\n",
    "V_pi = np.round(x.x, 1).reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22. , 24.4, 22. , 19.4, 17.5],\n",
       "       [19.8, 22. , 19.8, 17.8, 16. ],\n",
       "       [17.8, 19.8, 17.8, 16. , 14.4],\n",
       "       [16. , 17.8, 16. , 14.4, 13. ],\n",
       "       [14.4, 16. , 14.4, 13. , 11.7]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      "------------------------\n",
      "['r'] ['r', 'l', 'd', 'u'] ['l'] ['r', 'l', 'd', 'u'] ['l'] \n",
      "['r', 'u'] ['u'] ['l', 'u'] ['l'] ['l'] \n",
      "['r', 'u'] ['u'] ['l', 'u'] ['l', 'u'] ['l', 'u'] \n",
      "['r', 'u'] ['u'] ['l', 'u'] ['l', 'u'] ['l', 'u'] \n",
      "['r', 'u'] ['u'] ['l', 'u'] ['l', 'u'] ['l', 'u'] \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Optimal  Policy\n",
    "\"\"\"\n",
    "print (\"Optimal Policy\")\n",
    "print (\"------------------------\")\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        returns = []\n",
    "        for k in range(len(actions)):\n",
    "            next_pos, reward = step_and_reward(i, j, actions[k])\n",
    "            returns.append(V_pi.reshape(n*n)[next_pos])\n",
    "        sorted_returns = np.argsort(returns)\n",
    "        best_return = np.max(returns)\n",
    "        best_actions = sorted_returns[len(actions) - returns.count(best_return):]\n",
    "        best_actions = list(map(lambda x: action_names[str(x)], best_actions))\n",
    "        print (best_actions, end=\" \")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Iterative Value evaluation. Non-Linear equations solved in later cells\n",
    "\"\"\"\n",
    "\n",
    "grid = np.zeros((n, n))\n",
    "\n",
    "delta = 1e-4\n",
    "diff = 1000\n",
    "while diff > delta:\n",
    "    updated_grid = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            maxvalue = 0\n",
    "            for action in actions: # four actions possible\n",
    "                if (i,j) == A: # Give priority to A and B since all moves are valid here regardless of bounds\n",
    "                    reward = 10.0\n",
    "                    x,y = A1\n",
    "                elif (i,j) == B:\n",
    "                    reward = 5.0\n",
    "                    x,y = B1\n",
    "                elif i + action[0] < 0 or i + action[0] >= n or j + action[1] < 0 or j + action[1] >= n: # Check bounds after checking for A and B\n",
    "                    reward = -1.0\n",
    "                    x,y = i,j\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    x,y = i + action[0], j + action[1]\n",
    "                # Bellman's equation\n",
    "                maxvalue = max(maxvalue, (reward + gamma * (grid[x, y])))\n",
    "            updated_grid[i, j] += maxvalue\n",
    "    diff = np.abs(np.sum(np.subtract(updated_grid, grid)))\n",
    "    grid = updated_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.97744338 24.41938153 21.97744338 19.41938153 17.47744338]\n",
      " [19.77969904 21.97744338 19.77969904 17.80172914 16.02153504]\n",
      " [17.80172914 19.77969904 17.80172914 16.02153504 14.41938153]\n",
      " [16.02153504 17.80172914 16.02153504 14.41938153 12.97744338]\n",
      " [14.41938153 16.02153504 14.41938153 12.97744338 11.67969904]]\n"
     ]
    }
   ],
   "source": [
    "print (grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
